{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best screenplay - motion picture',\n",
       " 'best director - motion picture',\n",
       " 'best performance by an actress in a television series - comedy or musical',\n",
       " 'best foreign language film',\n",
       " 'best performance by an actor in a supporting role in a motion picture',\n",
       " 'best performance by an actress in a supporting role in a series, mini-series or motion picture made for television',\n",
       " 'best motion picture - comedy or musical',\n",
       " 'best performance by an actress in a motion picture - comedy or musical',\n",
       " 'best mini-series or motion picture made for television',\n",
       " 'best original score - motion picture',\n",
       " 'best performance by an actress in a television series - drama',\n",
       " 'best performance by an actress in a motion picture - drama',\n",
       " 'cecil b. demille award',\n",
       " 'best performance by an actor in a motion picture - comedy or musical',\n",
       " 'best motion picture - drama',\n",
       " 'best performance by an actor in a supporting role in a series, mini-series or motion picture made for television',\n",
       " 'best performance by an actress in a supporting role in a motion picture',\n",
       " 'best television series - drama',\n",
       " 'best performance by an actor in a mini-series or motion picture made for television',\n",
       " 'best performance by an actress in a mini-series or motion picture made for television',\n",
       " 'best animated feature film',\n",
       " 'best original song - motion picture',\n",
       " 'best performance by an actor in a motion picture - drama',\n",
       " 'best television series - comedy or musical',\n",
       " 'best performance by an actor in a television series - drama',\n",
       " 'best performance by an actor in a television series - comedy or musical']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# df = pd.read_csv(Path(globals()['_dh'][0])/'Data'/'csvFile')\n",
    "gg2013answers_path = os.path.join(os.path.dirname(os.path.abspath('')), 'data', 'gg2013answers.json')\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(gg2013answers_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "gg2013_award_names = list(data['award_data'].keys())\n",
    "gg2013_award_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from ftfy import fix_text\n",
    "import unidecode\n",
    "import json\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Fix encoding issues (ampersands, etc.) using ftfy\n",
    "    text = fix_text(text)\n",
    "    \n",
    "    # Remove non-ASCII characters (emojis, unicode symbols) using unidecode\n",
    "    text = unidecode.unidecode(text)\n",
    "    \n",
    "    # Remove extra whitespace, tabs, and newlines (substitute with single spaces)\n",
    "    # If we want to keep tabs/newline characters: text = re.sub(' +', ' ', text)\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_hashtags_and_links(text):\n",
    "    # Extract hashtags and links\n",
    "    hashtags = re.findall(r'#\\w+', text)  # Extract hashtags\n",
    "    links = re.findall(r'http[s]?://\\S+', text)  # Extract URLs\n",
    "    \n",
    "    # Remove hashtags and links from the original text\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)  # Remove URLs\n",
    "    \n",
    "    # Remove extra whitespace, tabs, and newlines (substitute with single spaces)\n",
    "    # If we want to keep tabs/newline characters: text = re.sub(' +', ' ', text)\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text, hashtags, links\n",
    "\n",
    "def preprocess_tweets(filename):\n",
    "    df = pd.read_json(filename)\n",
    "\n",
    "    # Extract user information into separate columns\n",
    "    df['user_screen_name'] = df['user'].apply(lambda x: x['screen_name'])\n",
    "    df['user_id'] = df['user'].apply(lambda x: x['id'])\n",
    "\n",
    "    # Drop the original 'user' column as we've extracted the needed information\n",
    "    df = df.drop('user', axis=1)\n",
    "\n",
    "    # Convert timestamp_ms to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp_ms'], unit='ms')\n",
    "\n",
    "    # Drop the original timestamp_ms column\n",
    "    df = df.drop('timestamp_ms', axis=1)\n",
    "\n",
    "    # Reorder columns for better readability\n",
    "    df = df[['id', 'timestamp', 'user_id', 'user_screen_name', 'text']]\n",
    "\n",
    "    # Apply preprocessing\n",
    "    df['clean_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "    # Display new text\n",
    "    df['clean_text'].head()\n",
    "\n",
    "    # Apply preprocessing\n",
    "    df[['cleaned_text', 'hashtags', 'links']] = df['text'].apply(\n",
    "        lambda x: pd.Series(extract_hashtags_and_links(x))\n",
    "    )\n",
    "\n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values(by='timestamp')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predictions_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to apply regex patterns and extract potential winners\n",
    "def extract_potential_winners(text, award):\n",
    "    # Improved regex to properly handle 'just' variations\n",
    "    just_variations = r'(?:(?:(?:she|he)\\s+)?just\\s+)?'\n",
    "    winner_patterns = [\n",
    "        r'(\\w+(?:\\s+\\w+)?)\\s+' + just_variations + r'wins\\s+(?!' + award + ')',\n",
    "        r'(\\w+(?:\\s+\\w+)?)\\s+' + just_variations + r'won\\s+(?!' + award + ')',\n",
    "        r'(\\w+(?:\\s+\\w+)?)\\s+' + just_variations + r'awarded\\s+(?!' + award + ')',\n",
    "        r'(\\w+(?:\\s+\\w+)?)\\s+' + just_variations + r'receives\\s+(?!' + award + ')',\n",
    "        r'(\\w+(?:\\s+\\w+)?)\\s+' + just_variations + r'received\\s+(?!' + award + ')'\n",
    "    ]\n",
    "    winners = []\n",
    "    for pattern in winner_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        winners.extend(matches)\n",
    "    return winners\n",
    "\n",
    "def extract_all_winners(df, award, nominees=[], presenters=[]):\n",
    "    '''\n",
    "    Returns a JSON with the information about the award, and a list of winners and the number of tweets they were mentioned in as a winner. \n",
    "\n",
    "    Example output: \n",
    "    {\n",
    "        \"Award\": \"Best Picture\",\n",
    "        \"Nominees\": [\"Nominee 1\", \"Nominee 2\", \"Nominee 3\", \"Nominee 4\", \"Nominee 5\"], \n",
    "        \"Presenters\": [\"Presenter 1\", \"Presenter 2\", \"Presenter 3\"],\n",
    "        \"Winners\": [\n",
    "            {\n",
    "                \"Name\": Winner 1,\n",
    "                \"Number of Tweets\": 512\n",
    "            },\n",
    "            {\n",
    "                \"Name\": Winner 2,\n",
    "                \"Number of Tweets\": 123\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    '''\n",
    "    # Apply the extraction function to the 'text' column\n",
    "    df['potential_winners'] = df['clean_text'].apply(lambda x: extract_potential_winners(x, award))\n",
    "\n",
    "    # Print all non-NaN values in the potential_winners column\n",
    "    all_winners = df['potential_winners'].dropna()\n",
    "    winner_counts = {}\n",
    "    for winners in all_winners:\n",
    "        if winners:  # Check if the list is not empty\n",
    "            for winner in winners:\n",
    "                if winner in winner_counts:\n",
    "                    winner_counts[winner] += 1\n",
    "                else:\n",
    "                    winner_counts[winner] = 1\n",
    "\n",
    "    # Create the JSON structure\n",
    "    output = {\n",
    "        \"Award\": award,\n",
    "        \"Nominees\": nominees,  # We don't have nominee information in the current data\n",
    "        \"Presenters\": presenters,  # We don't have presenter information in the current data\n",
    "        \"Winners\": [\n",
    "            {\n",
    "                \"Name\": winner,\n",
    "                \"Number of Tweets\": count\n",
    "            } for winner, count in winner_counts.items()\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Sort the winners by number of tweets in descending order\n",
    "    output[\"Winners\"] = sorted(output[\"Winners\"], key=lambda x: x[\"Number of Tweets\"], reverse=True)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal(s):\n",
    "- Map names/nicknames to entities -> map should contain entity names in some standard format (First, Last) \n",
    "- Map entities to some quantity of popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Entity lists to check against:\n",
    "- Names\n",
    "- Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "\n",
    "# ENTITIES = [\n",
    "#     'Ben Affleck', \n",
    "#     'Anne Hathaway', \n",
    "#     'Julianne Moore', \n",
    "#     'Adele', \n",
    "#     'Jessica Chastain', \n",
    "#     'Daniel Day-Lewis', \n",
    "#     'Denzel Washington', \n",
    "#     'Jonah Hill', \n",
    "#     'Brad Pitt', \n",
    "#     'Amy Poehler'\n",
    "# ]\n",
    "\n",
    "def create_cast_crew_df(year):    \n",
    "    movies_credits_df = create_movies_credits_df(year)\n",
    "    \n",
    "    # Schema: title, character, gender, name, order (of appearance)\n",
    "    cast_df = movies_credits_df[['title', 'cast']]\n",
    "    cast_df = cast_df.explode('cast')\n",
    "    cast_df['cast'].apply(pd.Series)\n",
    "    cast_df = pd.concat([cast_df, cast_df['cast'].apply(pd.Series)], axis=1).drop('cast', axis=1)\n",
    "\n",
    "    # Schema: title, job, name\n",
    "    crew_df = movies_credits_df[['title', 'crew']]\n",
    "    crew_df = crew_df.explode('crew')\n",
    "    crew_df['crew'].apply(pd.Series)\n",
    "    crew_df = pd.concat([crew_df, crew_df['crew'].apply(pd.Series)], axis=1).drop('crew', axis=1)\n",
    "\n",
    "    return cast_df, crew_df\n",
    "\n",
    "# create combined df w/ movies & credits\n",
    "def create_movies_credits_df(year):\n",
    "    # declare file paths\n",
    "    movies_metadata_path = os.path.join(os.path.dirname(os.path.abspath('')), 'data', 'movies_metadata.csv')\n",
    "    credits_path = os.path.join(os.path.dirname(os.path.abspath('')), 'data', 'credits.csv')\n",
    "\n",
    "    movies = pd.read_csv(movies_metadata_path)\n",
    "    credits = pd.read_csv(credits_path)\n",
    "\n",
    "    # filter to 'Released' movies only\n",
    "    movies = movies[movies['status']=='Released']\n",
    "    \n",
    "    # remove unnecessary columns\n",
    "    movies.drop(columns=['belongs_to_collection', 'budget', 'homepage', 'imdb_id', 'overview', 'poster_path', 'runtime', 'status', 'tagline', 'video'], inplace=True)\n",
    "    # function to check int id types\n",
    "    def is_integer(val):\n",
    "        try:\n",
    "            # try to convert to int\n",
    "            int(val)\n",
    "            return True\n",
    "        except (ValueError, TypeError):\n",
    "            return False\n",
    "\n",
    "    # filter rows where 'id' is an integer-like value\n",
    "    movies = movies[movies.id.apply(is_integer)]\n",
    "\n",
    "    # convert 'id' column to int\n",
    "    movies.id = movies.id.astype(int)\n",
    "\n",
    "    # merge with credits df\n",
    "    df = pd.merge(movies, credits, on='id')\n",
    "    df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "    # clean columns\n",
    "    cols = ['genres', 'production_companies', 'production_countries', 'spoken_languages']\n",
    "    for col in cols:\n",
    "        df[col] = df[col].apply(extract_names)\n",
    "    df.release_date = pd.to_datetime(df.release_date)\n",
    "    df.cast = df.cast.apply(clean_cast_data)\n",
    "    df.crew = df.crew.apply(clean_crew_data)\n",
    "\n",
    "    # filter movie/credit data for relevant year\n",
    "    df = df[df['release_date'].dt.year == year]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# extract the category names\n",
    "def extract_names(name_str):\n",
    "    if pd.isna(name_str):\n",
    "        return []\n",
    "    # convert the string representation of the list to an actual list\n",
    "    str_list = ast.literal_eval(name_str)\n",
    "    # extract the 'name' from each dictionary in the list\n",
    "    names = [i['name'] for i in str_list]\n",
    "    # return list of names as a string\n",
    "    return ', '.join(names)\n",
    "\n",
    "# clean the cast data\n",
    "def clean_cast_data(cast_str):\n",
    "    # convert string representation of the list to an actual list\n",
    "    cast_list = ast.literal_eval(cast_str)\n",
    "\n",
    "    # extract relevant fields and change gender values\n",
    "    cleaned_cast = []\n",
    "    for member in cast_list:\n",
    "        cleaned_member = {\n",
    "            'character': member['character'],\n",
    "            'gender': 'm' if member['gender'] == 2 else 'f' if member['gender'] == 1 else None,\n",
    "            'name': member['name'],\n",
    "            'order': member['order']\n",
    "        }\n",
    "        cleaned_cast.append(cleaned_member)\n",
    "    return cleaned_cast\n",
    "\n",
    "# clean the crew data\n",
    "def clean_crew_data(crew_str):\n",
    "    # convert string representation of the list to an actual list\n",
    "    crew_list = ast.literal_eval(crew_str)\n",
    "\n",
    "    # extract relevant fields\n",
    "    cleaned_crew = []\n",
    "    for member in crew_list:\n",
    "        cleaned_member = {\n",
    "            'job': member['job'],\n",
    "            'name': member['name']\n",
    "        }\n",
    "        cleaned_crew.append(cleaned_member)\n",
    "    return cleaned_crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jimin\\AppData\\Local\\Temp\\ipykernel_19496\\813785861.py:41: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  movies = pd.read_csv(movies_metadata_path)\n"
     ]
    }
   ],
   "source": [
    "cast_df, crew_df = create_cast_crew_df(2013)\n",
    "\n",
    "# combine distinct names into list - one for movies, one for people\n",
    "titles = cast_df['title'].unique()\n",
    "cast_names = list(cast_df['name'].unique())\n",
    "crew_names = list(crew_df['name'].unique())\n",
    "\n",
    "# ENTITY LISTS\n",
    "movie_entities = list(titles)\n",
    "people_entities = set(cast_names + crew_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLUSTERING**\n",
    "\n",
    "Many names may be associated with a given entity\n",
    "- Identify names \"similar\" to the entity (ex: Anne Hathaway - anne hathaway, @annehathaway, etx )\n",
    "- Note that not every string may be mapped to an entity\n",
    "\n",
    "Quantifing \"similarity\" between strings via different distance metrics:\n",
    "- token overlap -> # times each word in string appears in each defined entity, return highest entity (https://stackoverflow.com/questions/10136077/python-natural-language-processing-for-named-entities)\n",
    "- loads of string metrics (https://en.wikipedia.org/wiki/String_metric) -> levenshtein, hamming, jaccard, etx (https://www.nltk.org/api/nltk.metrics.html#module-nltk.metrics.distance)\n",
    "- considerations for string metrics: some metrics require comparison of strings of identical length (ex: Hamming dist.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "def compute_edit_distance(string, entity_list):\n",
    "    '''\n",
    "    For a given string, compute edit distances against all possible entities\n",
    "    Returns most similar matches from defined entity list\n",
    "    '''\n",
    "    entity_similarity_dict = {} # entity : similarity_score\n",
    "\n",
    "    for entity in entity_list:\n",
    "        # print(entity)\n",
    "        try:\n",
    "            similarity = edit_distance(string.lower(), entity.lower(), transpositions=True)\n",
    "            # print(f\"Entity: {entity} | Similarity: {similarity}\")\n",
    "            entity_similarity_dict[entity] = similarity\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return sorted( ((v,k) for k,v in entity_similarity_dict.items())) \n",
    "\n",
    "\n",
    "def token_overlap(query_string, classes):\n",
    "    \"\"\"\n",
    "    Computes the most \"likely\" class for the given query string.\n",
    "\n",
    "    First normalises the query to lower case, then computes the number of\n",
    "    overlapping tokens for each of the possible classes.\n",
    "\n",
    "    The class(es) with the highest overlap are returned as a list.\n",
    "\n",
    "    \"\"\"\n",
    "    query_tokens = query_string.lower().split() # lowercase query\n",
    "    class_tokens = [[x.lower() for x in c.split()] for c in classes] # lowercase each class in CLASSES\n",
    "    # print(f\"tokens:{class_tokens}\")\n",
    "\n",
    "\n",
    "    overlap = [0] * len(classes) # num times each word in query string appears for each defined CLASS \n",
    "    # check overlap on word/token level, not char\n",
    "    for token in query_tokens:\n",
    "        for index in range(len(classes)): \n",
    "            if token in class_tokens[index]:\n",
    "                overlap[index] += 1\n",
    "\n",
    "    # print(overlap)\n",
    "\n",
    "    sorted_overlap = [(count, index) for index, count in enumerate(overlap)]\n",
    "    sorted_overlap.sort()\n",
    "    sorted_overlap.reverse()\n",
    "\n",
    "    best_count = sorted_overlap[0][0]\n",
    "\n",
    "    best_classes = []\n",
    "    for count, index in sorted_overlap:\n",
    "        if count == best_count and count > 0: # count > 0 -> DON'T FORCE MAPPING IF NO OVERLAP WITH ANY ENTITY\n",
    "            best_classes.append(classes[index]) # (classes[index], count) to get token overlap count\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return best_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregate**\n",
    "\n",
    "Given potential winners, return top N candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_candidates(potential_winners, entity_list, top_n=5):\n",
    "    # data structure -> entity : count\n",
    "    entity_count = {} \n",
    "\n",
    "    # LIMITING SEARCH TO TOP 50 CANDIDATES\n",
    "    winners = sorted(potential_winners[\"Winners\"][:20], key=lambda x: x[\"Number of Tweets\"], reverse=True)\n",
    "\n",
    "    # traverse names in winners\n",
    "    for i in range(len(winners)):\n",
    "        winner_info = winners[i] # name & tweet count\n",
    "        winner_name = winner_info[\"Name\"]\n",
    "        winner_count = winner_info[\"Number of Tweets\"]\n",
    "        \n",
    "        # identify entities \"closest\" to winner_name - quantified via similarity metric\n",
    "        best_matches = compute_edit_distance(winner_name, entity_list=entity_list)[:top_n]\n",
    "        best_match = best_matches[0][1] # [0] for top match, [1] for name\n",
    "        \n",
    "        # map name to entity, update entity count\n",
    "        if best_match in entity_count:\n",
    "            entity_count[best_match] += winner_count\n",
    "        else:\n",
    "            entity_count[best_match] = winner_count  \n",
    "\n",
    "    # winner = entity w/ highest count\n",
    "    candidate_dict = dict(sorted(entity_count.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    nominees = list(candidate_dict.keys())[:top_n]\n",
    "    winner = nominees[0]\n",
    "    nominees.remove(winner)\n",
    "    \n",
    "    return nominees, winner\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main.py pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWARD: best director - motion picture\n",
      "Nominees: ['Anne Hathaway', 'Hugh Jackman', 'Jennifer Lawrence', 'Ali']\n",
      "Winner: Ben Affleck\n"
     ]
    }
   ],
   "source": [
    "award_name = 'best director - motion picture'\n",
    "# ground truth data -> data['award_data'][award_name]\n",
    "\n",
    "gg2013_path = os.path.join(os.path.dirname(os.path.abspath('')), 'data', 'gg2013.json')\n",
    "\n",
    "df = preprocess_tweets(gg2013_path)\n",
    "\n",
    "potential_award_winners = extract_all_winners(df, award=award_name, nominees=[], presenters=[])\n",
    "\n",
    "nominees, winner = aggregate_candidates(potential_winners=potential_award_winners, entity_list=people_entities)\n",
    "\n",
    "print(f\"AWARD: {award_name}\")\n",
    "print(f\"Nominees: {nominees}\")\n",
    "print(f\"Winner: {winner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nominees': ['kathryn bigelow',\n",
       "  'ang lee',\n",
       "  'steven spielberg',\n",
       "  'quentin tarantino'],\n",
       " 'presenters': ['halle berry'],\n",
       " 'winner': 'ben affleck'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['award_data'][award_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type checking movie vs. human awards)\n",
    "- just check whether candidate entity is human\n",
    "\n",
    "Type checking against movies/credits data)\n",
    "- data contains many movies w/ cast & crew for each of those movies\n",
    "- winners/nominees must be entities that exist in the cast/crew for those movies\n",
    "    - extract cast for each movie (top N?) and store in db\n",
    "    - check against this list when identifying entities\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweet_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
