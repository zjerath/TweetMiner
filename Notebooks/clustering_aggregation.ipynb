{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "with open('data/gg2013.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert the JSON data to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract user information into separate columns\n",
    "df['user_screen_name'] = df['user'].apply(lambda x: x['screen_name'])\n",
    "df['user_id'] = df['user'].apply(lambda x: x['id'])\n",
    "\n",
    "# Drop the original 'user' column as we've extracted the needed information\n",
    "df = df.drop('user', axis=1)\n",
    "\n",
    "# Convert timestamp_ms to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp_ms'], unit='ms')\n",
    "\n",
    "# Drop the original timestamp_ms column\n",
    "df = df.drop('timestamp_ms', axis=1)\n",
    "\n",
    "# Reorder columns for better readability\n",
    "df = df[['id', 'timestamp', 'user_id', 'user_screen_name', 'text']]\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "# print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to apply regex patterns and extract potential winners\n",
    "def extract_winners(text, award):\n",
    "    # Improved regex to properly handle 'just' variations\n",
    "    just_variations = r'(?:(?:(?:she|he)\\s+)?just\\s+)?'\n",
    "    winner_patterns = [\n",
    "        r'(\\w+(?:\\s+\\w+)?)\\s+' + just_variations + r'wins\\s+(?!' + award + ')',\n",
    "        r'(\\w+(?:\\s+\\w+)?)\\s+' + just_variations + r'won\\s+(?!' + award + ')',\n",
    "        r'(\\w+(?:\\s+\\w+)?)\\s+' + just_variations + r'awarded\\s+(?!' + award + ')',\n",
    "        r'(\\w+(?:\\s+\\w+)?)\\s+' + just_variations + r'receives\\s+(?!' + award + ')',\n",
    "        r'(\\w+(?:\\s+\\w+)?)\\s+' + just_variations + r'received\\s+(?!' + award + ')'\n",
    "    ]\n",
    "    winners = []\n",
    "    for pattern in winner_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        winners.extend(matches)\n",
    "    return winners\n",
    "\n",
    "# Apply the extraction function to the 'text' column\n",
    "df['potential_winners'] = df['text'].apply(lambda x: extract_winners(x, \"Best Picture\"))\n",
    "\n",
    "# Print all non-NaN values in the potential_winners column\n",
    "all_winners = df['potential_winners'].dropna()\n",
    "winner_counts = {}\n",
    "for winners in all_winners:\n",
    "    if winners:  # Check if the list is not empty\n",
    "        for winner in winners:\n",
    "            if winner in winner_counts:\n",
    "                winner_counts[winner] += 1\n",
    "            else:\n",
    "                winner_counts[winner] = 1\n",
    "\n",
    "# Create the JSON structure\n",
    "output = {\n",
    "    \"Award\": \"Best Picture\",\n",
    "    \"Nominees\": [],  # We don't have nominee information in the current data\n",
    "    \"Presenters\": [],  # We don't have presenter information in the current data\n",
    "    \"Winner\": [\n",
    "        {\n",
    "            \"Name\": winner,\n",
    "            \"Number of Tweets\": count\n",
    "        } for winner, count in winner_counts.items()\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Sort the winners by number of tweets in descending order\n",
    "output[\"Winner\"] = sorted(output[\"Winner\"], key=lambda x: x[\"Number of Tweets\"], reverse=True)\n",
    "\n",
    "# print(json.dumps(output, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assume nothing about outputs\n",
    "\n",
    "from winners list, identify candidates who fit type restrictions\n",
    "\n",
    "store in nominees - check against 2013 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type constraints:\n",
    "- eliminate pronouns, random phrases\n",
    "\n",
    "Goal(s):\n",
    "- Map entities to names/nicknames -> map should contain entity names in some standard format (First, Last) \n",
    "- Map entities to some quantity of popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt at entity recognition from \"winners\" list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m winners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWinner\u001b[39m\u001b[38;5;124m\"\u001b[39m], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Tweets\u001b[39m\u001b[38;5;124m\"\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m spacy_model \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_lg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# better entity recognition capability than en_core_web_sm\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# {\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     Name : _,\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     Entities : []\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[1;32m     11\u001b[0m entity_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/TweetMiner/TweetMiner/lib/python3.11/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/TweetMiner/TweetMiner/lib/python3.11/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "winners = sorted(output[\"Winner\"], key=lambda x: x[\"Number of Tweets\"], reverse=True)\n",
    "\n",
    "spacy_model = spacy.load('en_core_web_lg') # better entity recognition capability than en_core_web_sm\n",
    "\n",
    "# {\n",
    "#     Name : _,\n",
    "#     Entities : []\n",
    "# }\n",
    "entity_list = []\n",
    "\n",
    "for i in range(len(winners)):\n",
    "    winner_name = winners[i][\"Name\"]\n",
    "    spacy_output = spacy_model(winner_name)\n",
    "    # print(f\"spacy output: {spacy_output.ents}\")\n",
    "    # if spacy_output.ents == (): print(\"NO ENTITY IDENTIFIED\")\n",
    "    associated_entities = []\n",
    "    for entity in spacy_output.ents:\n",
    "        # print(f\"entity:{entity}\")\n",
    "        # print([entity.text, entity.label_])\n",
    "        # entity_list.append(entity.text)\n",
    "        associated_entities.append(entity.text)\n",
    "    \n",
    "    name_entities = {\n",
    "        \"Name\" : winner_name,\n",
    "        \"Entities\" : associated_entities\n",
    "    }\n",
    "    \n",
    "    entity_list.append(name_entities)\n",
    "    \n",
    "for i in range(30): print(entity_list[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Entity List to check against:\n",
    "- hard coding testing purposes, eventually will need to extract these too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTITIES = []\n",
    "\n",
    "# for entry in entity_list:\n",
    "#     if len(entry['Entities']) > 0: ENTITIES.append(entry['Entities']) \n",
    "\n",
    "ENTITIES = [\n",
    "    'Ben Affleck', \n",
    "    'Anne Hathaway', \n",
    "    'Julianne Moore', \n",
    "    'Adele', \n",
    "    'Jessica Chastain', \n",
    "    'Daniel Day-Lewis', \n",
    "    'Denzel Washington', \n",
    "    'Jonah Hill', \n",
    "    'Brad Pitt', \n",
    "    'Amy Poehler'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLUSTERING**\n",
    "\n",
    "Many names may be associated with a given entity\n",
    "- Identify names \"similar\" to the entity (ex: Anne Hathaway - anne hathaway, @annehathaway, etx )\n",
    "- Note that not every string may be mapped to an entity\n",
    "\n",
    "Quantifing \"similarity\" between strings i.e. string distance?\n",
    "- token overlap -> # times each word in string appears in each defined entity, return highest entity\n",
    "- loads of string metrics (https://en.wikipedia.org/wiki/String_metric) -> levenshtein, hamming, jaccard, etx (https://www.nltk.org/api/nltk.metrics.html#module-nltk.metrics.distance)\n",
    "- considerations for string metrics: some metrics require comparison of strings of identical length (ex: Hamming dist.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Token Overlap\n",
    "- https://stackoverflow.com/questions/10136077/python-natural-language-processing-for-named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anne Hathaway']\n"
     ]
    }
   ],
   "source": [
    "def token_overlap(query_string, classes):\n",
    "    \"\"\"\n",
    "    Computes the most \"likely\" class for the given query string.\n",
    "\n",
    "    First normalises the query to lower case, then computes the number of\n",
    "    overlapping tokens for each of the possible classes.\n",
    "\n",
    "    The class(es) with the highest overlap are returned as a list.\n",
    "\n",
    "    \"\"\"\n",
    "    query_tokens = query_string.lower().split() # lowercase query\n",
    "    class_tokens = [[x.lower() for x in c.split()] for c in classes] # lowercase each class in CLASSES\n",
    "    # print(f\"tokens:{class_tokens}\")\n",
    "\n",
    "\n",
    "    overlap = [0] * len(classes) # num times each word in query string appears for each defined CLASS \n",
    "    # check overlap on word/token level, not char\n",
    "    for token in query_tokens:\n",
    "        for index in range(len(classes)): \n",
    "            if token in class_tokens[index]:\n",
    "                overlap[index] += 1\n",
    "\n",
    "    # print(overlap)\n",
    "\n",
    "    sorted_overlap = [(count, index) for index, count in enumerate(overlap)]\n",
    "    sorted_overlap.sort()\n",
    "    sorted_overlap.reverse()\n",
    "\n",
    "    best_count = sorted_overlap[0][0]\n",
    "\n",
    "    best_classes = []\n",
    "    for count, index in sorted_overlap:\n",
    "        if count == best_count and count > 0: # count > 0 -> DON'T FORCE MAPPING IF NO OVERLAP WITH ANY ENTITY\n",
    "            best_classes.append(classes[index]) # (classes[index], count) to get token overlap count\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return best_classes\n",
    "\n",
    "\n",
    "print(token_overlap(\"hathaway, anne\", classes=ENTITIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Ben Affleck | Similarity: 0.75\n",
      "Entity: Anne Hathaway | Similarity: 0.6\n",
      "Entity: Julianne Moore | Similarity: 0.7058823529411765\n",
      "Entity: Adele | Similarity: 0.9285714285714286\n",
      "Entity: Jessica Chastain | Similarity: 0.3076923076923077\n",
      "Entity: Daniel Day-Lewis | Similarity: 0.6470588235294118\n",
      "Entity: Denzel Washington | Similarity: 0.5294117647058824\n",
      "Entity: Jonah Hill | Similarity: 0.6666666666666666\n",
      "Entity: Brad Pitt | Similarity: 0.7333333333333333\n",
      "Entity: Amy Poehler | Similarity: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics import edit_distance, jaccard_distance\n",
    "\n",
    "str = \"chastain, jessica\"\n",
    "dist_measure = 0\n",
    "\n",
    "for entity in ENTITIES:\n",
    "    similarity = jaccard_distance(set([x for x in str]), set([x for x in entity]))\n",
    "    print(f\"Entity: {entity} | Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given some list of entities, map names in output to each entity\n",
    "\n",
    "Aggregate the counts for each name into count for that entity\n",
    "\n",
    "\"Winner\" is entity with highest count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 24 (2402185589.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 28\u001b[1;36m\u001b[0m\n\u001b[1;33m    if len(candidate_entities) == 0: continue\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 24\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "\n",
    "# data structure -> entity : [different names]\n",
    "entity_names = {key: [] for key in ENTITIES}\n",
    "\n",
    "# data structure -> entity : count\n",
    "entity_count = {key: 0 for key in ENTITIES} \n",
    "\n",
    "winners = sorted(output[\"Winner\"], key=lambda x: x[\"Number of Tweets\"], reverse=True)\n",
    "\n",
    "# traverse names in winners\n",
    "for i in range(len(winners)):\n",
    "    winner_info = winners[i] # name & tweet count\n",
    "    winner_name = winner_info[\"Name\"]\n",
    "    winner_count = winner_info[\"Number of Tweets\"]\n",
    "\n",
    "    # print(winner_info)\n",
    "    \n",
    "    # identify entities \"closest\" to winner_name - replace token_overlap w/ any similarity metric\n",
    "    candidate_entities = token_overlap(winner_name, classes=ENTITIES)         \n",
    "        \n",
    "    # don't map if no entity recognized\n",
    "    if len(candidate_entities) == 0: continue\n",
    "    \n",
    "    # typically single candidate identified, but in case multiple top candidates named pick random - should probably change\n",
    "    identified_entity = random.choice(candidate_entities) \n",
    "    \n",
    "    # print(f\"Name: {winner_name} | Candidate entities: {candidate_entities} | Identified entity: {identified_entity}\")\n",
    "\n",
    "    # map name to entity, update entity count\n",
    "    entity_names[identified_entity].append(winner_name)\n",
    "    entity_count[identified_entity] += winner_count\n",
    "\n",
    "    \n",
    "\n",
    "# winner = entity w/ highest count\n",
    "dict(sorted(entity_count.items(), key=lambda item: item[1], reverse=True))\n",
    "# print(entity_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ben Affleck': ['Ben Affleck',\n",
       "  'Affleck',\n",
       "  'Ben affleck',\n",
       "  'Ben',\n",
       "  'BEN AFFLECK',\n",
       "  'if Affleck',\n",
       "  'ben affleck',\n",
       "  'Affleck has',\n",
       "  'So Ben',\n",
       "  'BEN AFLECK',\n",
       "  'when Affleck',\n",
       "  'Ben Afleck',\n",
       "  'director Affleck',\n",
       "  'Ben just',\n",
       "  'happy Ben',\n",
       "  'that Affleck',\n",
       "  'affleck',\n",
       "  'ben afleck',\n",
       "  'Affleck just',\n",
       "  'glad Affleck',\n",
       "  'Ben Afflect',\n",
       "  'Glad Ben',\n",
       "  'thrilled Affleck',\n",
       "  'Affleck also'],\n",
       " 'Anne Hathaway': ['Anne Hathaway',\n",
       "  'ANNE HATHAWAY',\n",
       "  'Hathaway both',\n",
       "  'ANNE',\n",
       "  'pretend Hathaway',\n",
       "  'anne hathaway',\n",
       "  'Hathaway',\n",
       "  'Anne Hathway',\n",
       "  'Anne Hatheway',\n",
       "  'anne hatheway',\n",
       "  'Anne',\n",
       "  'Amne Hathaway',\n",
       "  'girl Anne',\n",
       "  'Hathaway to',\n",
       "  'Hathaway deserve',\n",
       "  'Hathaway immediately',\n",
       "  'and Anne',\n",
       "  'Hathaway have'],\n",
       " 'Julianne Moore': ['Julianne Moore',\n",
       "  'Julianna Moore',\n",
       "  'Julianne',\n",
       "  'Moore 2',\n",
       "  'Julian Moore',\n",
       "  'JULIANNE MOORE',\n",
       "  'now Julianne'],\n",
       " 'Adele': ['Adele',\n",
       "  'when Adele',\n",
       "  'by Adele',\n",
       "  'that Adele',\n",
       "  'adele',\n",
       "  'mum Adele',\n",
       "  'glad Adele',\n",
       "  'hope Adele',\n",
       "  'Adele even',\n",
       "  'Adele just',\n",
       "  '2013 ADELE',\n",
       "  'ADELE',\n",
       "  'happy Adele',\n",
       "  'as Adele',\n",
       "  'and Adele',\n",
       "  'GoldenGlobes Adele',\n",
       "  'And Adele',\n",
       "  'Yay Adele',\n",
       "  'Glad Adele',\n",
       "  'EVEN ADELE',\n",
       "  'RevistaBillboard Adele',\n",
       "  'bet Adele',\n",
       "  'Ew Adele',\n",
       "  'course Adele',\n",
       "  'YEEESSSSS ADELE',\n",
       "  'Ninang Adele',\n",
       "  'Yayyyy Adele',\n",
       "  'Go Adele',\n",
       "  'Adele has',\n",
       "  'GIRL ADELE',\n",
       "  'Word Adele',\n",
       "  'coarse Adele',\n",
       "  'that adele',\n",
       "  'Ahh Adele',\n",
       "  'Oh Adele',\n",
       "  'singer Adele',\n",
       "  'after Adele',\n",
       "  'so Adele',\n",
       "  'Obviously Adele',\n",
       "  'when  Adele',\n",
       "  'Hypable Adele',\n",
       "  'yPLzpLND Adele',\n",
       "  'GUESS ADELE',\n",
       "  'If Adele',\n",
       "  'Yeah Adele',\n",
       "  'TVGuide Adele',\n",
       "  'when adele',\n",
       "  'but Adele',\n",
       "  'jerrycferrara \\nAdele',\n",
       "  'glorious Adele',\n",
       "  'yea Adele',\n",
       "  'know adele',\n",
       "  'mom Adele',\n",
       "  'adele even',\n",
       "  'party Adele',\n",
       "  'OpG214Lb Adele'],\n",
       " 'Jessica Chastain': ['Jessica Chastain',\n",
       "  'Chastain has',\n",
       "  'Jessica Lange',\n",
       "  'Jessica Alba',\n",
       "  'jessica alba',\n",
       "  'If Chastain',\n",
       "  'Chastain',\n",
       "  'Jessica',\n",
       "  'jessica chastain',\n",
       "  'Chastain also'],\n",
       " 'Daniel Day-Lewis': ['Daniel Craig', 'Daniel DayLewis'],\n",
       " 'Denzel Washington': [],\n",
       " 'Jonah Hill': ['Hill still', 'Jonah Hill'],\n",
       " 'Brad Pitt': [],\n",
       " 'Amy Poehler': ['Amy Poehler',\n",
       "  'Poehler also',\n",
       "  'sure Amy',\n",
       "  'Poehler',\n",
       "  'amy poehler',\n",
       "  'Amy',\n",
       "  'Poehler still',\n",
       "  'Amy have',\n",
       "  'Fat Amy',\n",
       "  'Amy totally',\n",
       "  'and Poehler']}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweet_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
